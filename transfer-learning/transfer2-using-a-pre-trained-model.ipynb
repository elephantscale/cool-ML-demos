{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Using a pre-trained model\n",
    "\n",
    "We will use a pre-trained model to classify cats-and-dogs!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elephantscale/cool-ML-demos/blob/main/transfer-learning/transfer2-using-a-pre-trained-model.ipynb)\n",
    "\n",
    "### Runtime\n",
    "~ 30 minutes\n",
    "\n",
    "### Note\n",
    "Here we are dealing with real world images.  Processing them will required a lot of compute power.  \n",
    "If you have access to, switch to **GPU** as run time!\n",
    "\n",
    "### References\n",
    "- https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "- https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print ('tensorflow version :', tf.__version__)\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading our custom utils files\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Hack to download image utils when running on Colab ..etc\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/elephantscale/es-public/master/deep-learning/transfer_learning_utils.py'\n",
    "file_location = \"transfer_learning_utils.py\"\n",
    "\n",
    "if not os.path.exists (file_location):\n",
    "    file_location = os.path.basename(file_location)\n",
    "    if not os.path.exists(file_location):\n",
    "        print(\"Downloading : \", file_url)\n",
    "        urllib.request.urlretrieve(file_url, file_location)\n",
    "# print('file_location:', file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading our custom utils files\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Hack to download image utils when running on Colab ..etc\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/elephantscale/es-public/master/deep-learning/image_utils.py'\n",
    "file_location = \"image_utils.py\"\n",
    "\n",
    "if not os.path.exists (file_location):\n",
    "    file_location = os.path.basename(file_location)\n",
    "    if not os.path.exists(file_location):\n",
    "        print(\"Downloading : \", file_url)\n",
    "        urllib.request.urlretrieve(file_url, file_location)\n",
    "# print('file_location:', file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6sLPjRC3vPG"
   },
   "source": [
    "## TF-GPU Config\n",
    "The following cell sets TF properties to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block is to tweak TF running on GPU\n",
    "## You may comment this out, if you are not using GPU\n",
    "\n",
    "## ---- start Memory setting ----\n",
    "## Ask TF not to allocate all GPU memory at once.. allocate as needed\n",
    "## Without this the execution will fail with \"failed to initialize algorithm\" error\n",
    "\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\n",
    "## ---- end Memory setting ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Download Data\n",
    "We will use cat-dog-redux dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common constants\n",
    "\n",
    "IMG_WIDTH=160\n",
    "IMG_HEIGHT=160\n",
    "NUM_CLASSES=2\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_location = 'https://elephantscale-public.s3.amazonaws.com/data/images/cat-dog-redux.zip'\n",
    "\n",
    "data_location_local = keras.utils.get_file(fname=os.path.basename(data_location),\n",
    "                                           origin=data_location, extract=True)\n",
    "print ('local download file: ', data_location_local)\n",
    "data_dir = os.path.join(os.path.dirname(data_location_local), 'cat-dog-redux')\n",
    "print ('local data dir: ', data_dir)\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "validation_dir = os.path.join(data_dir, 'val')\n",
    "print ('train dir:', train_dir)\n",
    "print ('validation dir:', validation_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Download MobileNetv2 Model\n",
    "Here we will examine MobileNetv2 model.  MobileNetv2 was developed at Google.  It is pre-trained on the ImageNet dataset consisting of 1.4 Million images and 1000 classes.\n",
    "\n",
    "- paper: [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n",
    "- [github](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md)\n",
    "- [Tensorflow model page](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2)\n",
    "- [Tensorflow transfer learning guide](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
    "\n",
    "#### Notice the following:\n",
    "- To use the MobileNet model, input image dimension should be one of (w,w,3)  \n",
    "Image widths are one of 96, 128, 160, 192, 224(default).  \n",
    "We are using 160 as that is closest to our images\n",
    "- We are also initializing the model with the weights of 'imagenet'\n",
    "- We are importing the full model (including the classifiying layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transfer_learning_utils import print_model_summary_compact\n",
    "\n",
    "mobilenetv2_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_WIDTH,IMG_HEIGHT,3), \n",
    "                                               include_top = True,\n",
    "                                               weights = 'imagenet')\n",
    "print_model_summary_compact(mobilenetv2_model)\n",
    "\n",
    "# ! du -skh ~/.keras/models/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Predict on a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_single_image(image_file, model, model_pkg):\n",
    "    from tensorflow.keras.preprocessing.image import load_img\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.preprocessing.image import img_to_array\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    \n",
    "    print (\"image_file:\", image_file)\n",
    "    img = image.load_img(image_file, target_size = (IMG_WIDTH, IMG_HEIGHT))\n",
    "    # print (\"image size: \" , img.size)\n",
    "    \n",
    "    # scale the data\n",
    "    img_data = image.img_to_array(img)\n",
    "    # img_data = img_data / 255.0\n",
    "    # print ('image_data.shape:', img_data.shape)\n",
    "    \n",
    "    ## The networks accept a 4-dimensional Tensor as an input of the form \n",
    "    ## ( batchsize, height, width, channels).\n",
    "    img_data = np.expand_dims(img_data, axis = 0)\n",
    "    # print('image_data converted to tensor form (batchsize, height, width, channels):', img_data.shape)\n",
    "    \n",
    "    ## Preprocess the input by subtracting the mean value from each channel of the images in the batch. \n",
    "    ## Mean is an array of three elements obtained by the average of R, G, B pixels of all images obtained from ImageNet. \n",
    "    ## The values for ImageNet are : [ 103.939, 116.779, 123.68 ]. \n",
    "    ## This is done using the preprocess_input() function.\n",
    "    ## Using a copy here, because the function modifies the array\n",
    "    processed_image = model_pkg.preprocess_input(img_data.copy())\n",
    "    # print ('processed_image.shape:', processed_image.shape)\n",
    "\n",
    "    ## Finally ready to prodict\n",
    "    ## prediction will be a 1000 wide softmax array!\n",
    "    ## Remember, ImageNet has 1000 classes\n",
    "    prediction = model.predict (processed_image)\n",
    "    # prediction = mobilenetv2_model.predict (img_data)\n",
    "    # print ('prediction.shape:', prediction.shape)\n",
    "\n",
    "    ## pull the highest probability from softmax output\n",
    "    predictions2 = [ np.argmax(p) for p in prediction]\n",
    "    index_of_highest_probability = np.argmax(prediction[0])\n",
    "    value_of_highest_probability = prediction[0][index_of_highest_probability]\n",
    "    print (\"predicted class: \", index_of_highest_probability, \", probability:\", value_of_highest_probability)\n",
    "    \n",
    "    \n",
    "    ## This tells us a little about prediction\n",
    "    decoded_predictions = model_pkg.decode_predictions(prediction)\n",
    "\n",
    "    ## Here we are using a pandas dataframe to pretty-print top-5\n",
    "    print (\"Top-5 predictions:\")\n",
    "    df = pd.DataFrame(columns=['Class Id', 'Description', 'Probability (high to low)'])\n",
    "    for i, p in enumerate(decoded_predictions[0]):\n",
    "        # print (\"    \", i, p)\n",
    "        df.loc[i] = (p[0], p[1], p[2])\n",
    "\n",
    "    ## plot the image\n",
    "    # plt.figure()  # stack images vertically\n",
    "    print(df)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## End: predict_on_single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from image_utils import get_image_files_from_dir\n",
    "\n",
    "image_files = get_image_files_from_dir(validation_dir, recursive=True)\n",
    "# print (image_files)\n",
    "\n",
    "for i in range(5):\n",
    "    # get a random image\n",
    "    index = random.randint(0, len(image_files)-1)\n",
    "    image_file = image_files[index]\n",
    "    predict_on_single_image(image_file, mobilenetv2_model, tf.keras.applications.mobilenet)\n",
    "    print ('============================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Inception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=299\n",
    "IMG_HEIGHT=299\n",
    "\n",
    "from transfer_learning_utils import print_model_summary_compact\n",
    "\n",
    "inceptionv3_model = tf.keras.applications.InceptionV3(input_shape=(IMG_WIDTH,IMG_HEIGHT,3), \n",
    "                                               include_top = True,\n",
    "                                               weights = 'imagenet')\n",
    "print_model_summary_compact(inceptionv3_model)\n",
    "\n",
    "# ! du -skh ~/.keras/models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "image_files = get_image_files_from_dir(validation_dir, recursive=True)\n",
    "# print (image_files)\n",
    "\n",
    "for i in range(5):\n",
    "    # get a random image\n",
    "    index = random.randint(0, len(image_files)-1)\n",
    "    image_file = image_files[index]\n",
    "    predict_on_single_image(image_file, inceptionv3_model, tf.keras.applications.inception_v3)\n",
    "    print ('============================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=224\n",
    "IMG_HEIGHT=224\n",
    "\n",
    "from transfer_learning_utils import print_model_summary_compact\n",
    "\n",
    "resnet152v2_model = tf.keras.applications.ResNet152V2(input_shape=(IMG_WIDTH,IMG_HEIGHT,3), \n",
    "                                               include_top = True,\n",
    "                                               weights = 'imagenet')\n",
    "print_model_summary_compact(resnet152v2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "image_files = get_image_files_from_dir(validation_dir, recursive=True)\n",
    "# print (image_files)\n",
    "\n",
    "for i in range(5):\n",
    "    # get a random image\n",
    "    index = random.randint(0, len(image_files)-1)\n",
    "    image_file = image_files[index]\n",
    "    predict_on_single_image(image_file, resnet152v2_model, tf.keras.applications.resnet_v2)\n",
    "    print ('============================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
