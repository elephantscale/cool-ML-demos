{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_-rC05uU2Wk"
   },
   "source": [
    "# Lab: Text Generation with GPT2\n",
    "\n",
    "We will use pre-trained GPT2 model.  And generate text.\n",
    "\n",
    "Uses [Hugging Face Transfromer library](https://huggingface.co/transformers/)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elephantscale/cool-ML-demos/blob/main/transformers/GPT2_text_generation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWsczU8SOO7F",
    "outputId": "acc4116c-b367-4f5c-c7d0-03e6af3232ff"
   },
   "outputs": [],
   "source": [
    "## Install transformers -- one time only\n",
    "\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Dq0u-5TMOE5J",
    "outputId": "85d9ca78-117f-47e6-c060-58205c80e85f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SJBRXu9PQe4",
    "outputId": "7e4053f1-4094-49c7-b815-d816d98e8ce3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode a text inputs\n",
    "# text = \"Who was Jim Henson ? Jim Henson was a\"\n",
    "text_original = 'Antartica is a'\n",
    "text = text_original\n",
    "\n",
    "# tokenize\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "print (\"tokens_tensor : \", tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0xFQsKuPW8L",
    "outputId": "fd6981ee-6907-434c-865f-d3c5811d441d"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "device = 'cpu'\n",
    "#device = 'gpu'\n",
    "\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "model.to(device)\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "\n",
    "print ('input text : ', text)\n",
    "print ('predicted text :', predicted_text)\n",
    "\n",
    "# set text = predicted_text and run again\n",
    "text = predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwGAiX7UWKB6",
    "outputId": "9cd26f78-f75d-406d-8e8a-a10668c13446"
   },
   "outputs": [],
   "source": [
    "## Generate a sequence of sentences, feeding on themselves\n",
    "\n",
    "text = text_original\n",
    "\n",
    "for i in range (1,30):\n",
    "  indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "  # Convert indexed tokens in a PyTorch tensor\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  #print (\"tokens_tensor : \", tokens_tensor)\n",
    "\n",
    "  # Predict all tokens\n",
    "  with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "  # get the predicted next sub-word (in our case, the word 'man')\n",
    "  predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "  predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "  # assert predicted_text == 'Who was Jim Henson? Jim Henson was a man'\n",
    "\n",
    "  print ('input text : ', text)\n",
    "  print ('predicted text : ', predicted_text)\n",
    "  #print (predicted_text )\n",
    "  print()\n",
    "\n",
    "  text = predicted_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd3dfbB1W_87"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2_text_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
